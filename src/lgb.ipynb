{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_drop_features(features):\n",
    "    # id and label (not features)\n",
    "    unused_feature_list = ['price']\n",
    "\n",
    "    # Hurts performance\n",
    "    unused_feature_list += ['lat', 'long']\n",
    "\n",
    "    return features.drop(unused_feature_list, axis=1, errors='ignore')\n",
    "\n",
    "# Drop useless feature\n",
    "lgb_features = lgb_drop_features(df)\n",
    "print(\"Number of features for LGBM: {}\".format(len(lgb_features.columns)))\n",
    "lgb_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training and cross-validation data\n",
    "lgb_label = df.price.astype(np.float32)\n",
    "\n",
    "# Transform to Numpy matrices\n",
    "lgb_X = lgb_features\n",
    "lgb_y = pd.DataFrame(lgb_label)\n",
    "\n",
    "# Perform shuffled train/test split\n",
    "np.random.seed(42)\n",
    "random.seed(10)\n",
    "X_train, X_val, y_train, y_val = train_test_split(lgb_X, lgb_y, test_size=0.1)\n",
    "\n",
    "# Remove outlier examples from X_train and y_train; Keep them in X_val and y_val for proper cross-validation\n",
    "#outlier_threshold = 0.4\n",
    "#mask = (abs(y_train) <= outlier_threshold)\n",
    "#X_train = X_train[mask, :]\n",
    "#y_train = y_train[mask]\n",
    "\n",
    "print(\"X_train shape: {}\".format(X_train.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))\n",
    "print(\"X_val shape: {}\".format(X_val.shape))\n",
    "print(\"y_val shape: {}\".format(y_val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify feature names and categorical features for LightGBM\n",
    "feature_names = [s for s in lgb_features.columns]\n",
    "categorical_features = ['status', 'add_attr', 'state', 'year', 'fireplace', 'parking', \"subtype\", 'sewer', 'water', 'app', 'heating',\n",
    "       'cooling', 'materials', 'roof', 'foundation', 'interior']\n",
    "\n",
    "categorical_indices = []\n",
    "for i, n in enumerate(lgb_features.columns):\n",
    "    if n in categorical_features:\n",
    "        categorical_indices.append(i)\n",
    "print(categorical_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM parameters\n",
    "params = {}\n",
    "\n",
    "params['objective'] = 'regression'\n",
    "params['metric'] = 'mae'\n",
    "params['num_threads'] = 4                   # set to number of real CPU cores for best performance\n",
    "\n",
    "params['boosting_type'] = 'gbdt'\n",
    "params['num_boost_round'] = 2000\n",
    "params['learning_rate'] = 0.003             # shrinkage_rate\n",
    "params['early_stopping_rounds'] = 30        # Early stopping based on validation set performance\n",
    "\n",
    "# Control tree growing\n",
    "params['num_leaves'] = 127                  # max number of leaves in one tree (default 31)\n",
    "params['min_data'] = 150                    # min_data_in_leaf\n",
    "params['min_hessian'] = 0.001               # min_sum_hessian_in_leaf (default 1e-3)\n",
    "params['max_depth'] = -1                    # limit the max depth of tree model, default -1 (no limit)\n",
    "params['max_bin'] = 255                     # max number of bins that feature values are bucketed in (small -> less over fitting, default 255)\n",
    "params['sub_feature'] = 0.5                 # feature_fraction (small values => use very different sub models)\n",
    "\n",
    "# Row sub_sampling (speed up training and alleviate over fitting)\n",
    "params['bagging_fraction'] = 0.7\n",
    "params['bagging_freq'] = 50                 # perform bagging at every k iteration\n",
    "\n",
    "# Constraints on categorical features\n",
    "params['min_data_per_group'] = 100          # minimal number of data per categorical group (default 100)\n",
    "params['cat_smooth'] = 15.0                 # reduce effect of noises in categorical features, especially for those with few data (default 10.0)\n",
    "\n",
    "# Regularization (default 0.0)\n",
    "params['lambda_l1'] = 0.0\n",
    "params['lambda_l2'] = 0.0\n",
    "\n",
    "# Random seeds (keep default values)\n",
    "params['feature_fraction_seed'] = 2\n",
    "params['bagging_seed'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single LightGBM \n",
    "lgb_train_set = lgb.Dataset(X_train, label=y_train, feature_name=feature_names)\n",
    "lgb_valid_set = lgb.Dataset(X_val, label=y_val, feature_name=feature_names)\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(36)\n",
    "model = lgb.train(params, lgb_train_set, verbose_eval=False,\n",
    "                valid_sets=[lgb_train_set, lgb_valid_set], valid_names=['train', 'val'],\n",
    "                categorical_feature=categorical_indices)\n",
    "\n",
    "# Evaluate on train and validation sets\n",
    "print(\"Train score: {}\".format(abs(model.predict(X_train).reshape(-1,1) - y_train).mean()))\n",
    "print(\"Val score: {}\".format(abs(model.predict(X_val).reshape(-1,1) - y_val).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot LightGBM feature importance\n",
    "lgb.plot_importance(model, height=0.8, figsize=(12.5, 12.5), ignore_zero=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble 20x LightGBM\n",
    "lgb_train_set = lgb.Dataset(X_train, label=y_train, feature_name=feature_names)\n",
    "print(\"lgb_X: {}\".format(X_train.shape))\n",
    "print(\"lgb_y: {}\".format(y_train.shape))\n",
    "\n",
    "#del params['early_stopping_rounds']\n",
    "#del params['feature_fraction_seed']\n",
    "#del params['bagging_seed']\n",
    "params['num_boost_round'] = 2500\n",
    "\n",
    "# Train multiple models\n",
    "bags = 20\n",
    "models = []\n",
    "for i in range(bags):\n",
    "    print(\"Start training model {}\".format(i))\n",
    "    params['seed'] = i\n",
    "    np.random.seed(42)\n",
    "    random.seed(10)\n",
    "    model = lgb.train(params, lgb_train_set, verbose_eval=False, categorical_feature=categorical_indices)\n",
    "    models.append(model)\n",
    "    \n",
    "# Sanity check (make sure scores on a small portion of the dataset are reasonable)\n",
    "for i, model in enumerate(models):\n",
    "    print(\"model {}: {}\".format(i, abs(model.predict(X_val).reshape(-1,1) - y_val).mean()))\n",
    "\n",
    "# Save the trained models to disk\n",
    "# save_models(models)\n",
    "\n",
    "# models = load_models(['checkpoints/lgb_' + str(i) for i in range(5)])  # load pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, model in enumerate(models):\n",
    "    print(\"model {}: {}\".format(i, abs(model.predict(X_val).reshape(-1,1) - y_val).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_export(models, features, file_name):\n",
    "    # Construct DataFrame for prediction results\n",
    "    lgb = pd.DataFrame()\n",
    "\n",
    "    lgb['Index'] = features.index\n",
    "    \n",
    "    test_features = lgb_drop_features(features)\n",
    "    \n",
    "    pred = []\n",
    "    for i, model in enumerate(models):\n",
    "        print(\"Start model {}\".format(i))\n",
    "        pred.append(model.predict(test_features))\n",
    "    \n",
    "    # Take average across all models\n",
    "    mean_pred = np.mean(pred, axis=0)\n",
    "    \n",
    "    lgb['price'] = [float(format(x, '.4f')) for x in mean_pred]    \n",
    "    \n",
    "    print(\"Length of submission DataFrame: {}\".format(len(lgb)))\n",
    "    print(\"Submission header:\")\n",
    "    #lgb.to_csv(file_name, index=False)\n",
    "    return lgb, pred  # Return the results so that we can analyze or sanity check it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"/Users/charles/Desktop/DSAI/6_DataScience/project/DSP/20221/data/final_lgbm_ensemble_x20-2500.csv\"\n",
    "lgb, pred = predict_and_export(models, X_val, file_name)\n",
    "lgb.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
